{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85666f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyojin0912/anaconda3/envs/DeepREAL/lib/python3.6/site-packages/Bio/SubsMat/__init__.py:131: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  BiopythonDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# BioPython imports\n",
    "from Bio.PDB import MMCIFParser, PDBParser, Superimposer\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "from Bio import pairwise2\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "\n",
    "# --- Configuration ---\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = SCRIPT_DIR.parent\n",
    "CIF_DIR = PROJECT_ROOT / \"Data/CIF_Files/\"\n",
    "AF_DIR = PROJECT_ROOT / \"Data/AF_PDB/\"\n",
    "INPUT_DIR = PROJECT_ROOT / \"Input/\"\n",
    "OUTPUT_DIR_BASE = PROJECT_ROOT / \"Output/\"\n",
    "\n",
    "REP_CHAIN_FILE = OUTPUT_DIR_BASE / \"Binding_Residue/Rep_GPCR_chain.csv\"\n",
    "CLASSIFICATION_FILE = OUTPUT_DIR_BASE / \"Dynamics/GPCR_PDB_classification.csv\"\n",
    "REP_APO_FILE = OUTPUT_DIR_BASE / \"Final/Representative_Apo_Structures.csv\"\n",
    "SEQUENCE_INFO_FILE = INPUT_DIR / 'Human_GPCR_PDB_Info.csv'\n",
    "CLASS_INFO_FILE = INPUT_DIR / 'ChEMBL_GPCR_Info.csv'\n",
    "\n",
    "OUTPUT_DIR = OUTPUT_DIR_BASE / \"Decision_Tree/\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FEATURE_MATRIX_FILE = OUTPUT_DIR / \"GPCR_feature_matrix_for_tree.csv\"\n",
    "LOG_FILE = OUTPUT_DIR / \"feature_extraction_log.txt\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, mode='w'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8582b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS (Unchanged and verified)\n",
    "# ==============================================================================\n",
    "def get_bw_map_from_gpcrdb(entry_name):\n",
    "    url = f\"https://gpcrdb.org/services/residues/{entry_name.lower()}/\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        residues = response.json()\n",
    "        if residues:\n",
    "            return { res['sequence_number']: res['display_generic_number'].split('x')[0]\n",
    "                for res in residues if res.get('display_generic_number') }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.warning(f\"GPCRdb API call failed for {entry_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_rep_chain_id(uniprot_id, pdb_id, df_rep_chain):\n",
    "    subset = df_rep_chain[(df_rep_chain['UniProt_ID'] == uniprot_id) & (df_rep_chain['PDB_ID'] == pdb_id)]\n",
    "    if subset.empty: return None\n",
    "    return subset.sort_values(by='score', ascending=False).iloc[0]['chain_id']\n",
    "\n",
    "def load_structure_and_map(pdb_id, chain_id, uniprot_seq, is_alphafold=False):\n",
    "    if is_alphafold:\n",
    "        path = AF_DIR / f\"AF-{pdb_id}-F1-model_v3.pdb\"\n",
    "        parser = PDBParser(QUIET=True)\n",
    "    else:\n",
    "        path = CIF_DIR / f\"{pdb_id.lower()}.cif\"\n",
    "        parser = MMCIFParser(QUIET=True)\n",
    "    if not path.exists():\n",
    "        logging.error(f\"Structure file not found: {path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        structure = parser.get_structure(str(path.name), path)\n",
    "        chain_obj = structure[0][chain_id]\n",
    "        pdb_residues_with_id = {res.id[1]: res for res in chain_obj.get_residues() if res.id[0] == ' ' and 'CA' in res}\n",
    "        if not pdb_residues_with_id: return None, None\n",
    "        sorted_pdb_keys = sorted(pdb_residues_with_id.keys())\n",
    "        pdb_seq = \"\".join([three_to_one(pdb_residues_with_id[res_id].get_resname()) for res_id in sorted_pdb_keys])\n",
    "        alignments = pairwise2.align.localds(uniprot_seq, pdb_seq, matlist.blosum62, -10, -0.5)\n",
    "        if not alignments: return None, None\n",
    "        uniprot_to_pdb_residue_map = {}\n",
    "        pdb_key_idx, uni_pos_idx = 0, 0\n",
    "        for uni_char, pdb_char in zip(alignments[0].seqA, alignments[0].seqB):\n",
    "            if uni_char != '-': uni_pos_idx += 1\n",
    "            if pdb_char != '-':\n",
    "                if uni_char != '-' and pdb_key_idx < len(sorted_pdb_keys):\n",
    "                    pdb_res_id = sorted_pdb_keys[pdb_key_idx]\n",
    "                    uniprot_to_pdb_residue_map[uni_pos_idx] = pdb_residues_with_id[pdb_res_id]\n",
    "                pdb_key_idx += 1\n",
    "        return structure, uniprot_to_pdb_residue_map\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load or parse {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def get_representative_apo(uniprot_id, df_rep_apo):\n",
    "    subset = df_rep_apo[df_rep_apo['UniProt_ID'] == uniprot_id].copy()\n",
    "    covered = subset[subset['Binding_Coverage'] == 100.0]\n",
    "    if not covered.empty:\n",
    "        return covered.sort_values(by='Resolution', ascending=True).iloc[0]['PDB_ID']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fd2cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. CORE FEATURE CALCULATION WORKFLOW (CRITICAL BUG FIXED)\n",
    "# ==============================================================================\n",
    "def process_gpcr(gpcr_row, df_rep_chain, df_seq_info, ref_apo_map, bw_cache):\n",
    "    uniprot_id = gpcr_row['GPCR']\n",
    "    uniprot_info = df_seq_info[df_seq_info['Entry'] == uniprot_id].iloc[0]\n",
    "    uniprot_seq, entry_name = uniprot_info['Sequence'], uniprot_info['Entry Name']\n",
    "    if uniprot_id not in bw_cache: bw_cache[uniprot_id] = get_bw_map_from_gpcrdb(entry_name)\n",
    "    uniprot_to_bw = bw_cache.get(uniprot_id)\n",
    "    if not uniprot_to_bw: return [], bw_cache\n",
    "    bw_to_uniprot = {bw: pos for pos, bw in uniprot_to_bw.items()}\n",
    "    uniprot_pos_R3_50 = bw_to_uniprot.get('3.50')\n",
    "    if not uniprot_pos_R3_50: return [], bw_cache\n",
    "    ref_pdb_id = ref_apo_map.get(uniprot_id)\n",
    "    is_ref_alphafold = ref_pdb_id is None\n",
    "    ref_pdb_id_for_loading = uniprot_id if is_ref_alphafold else ref_pdb_id\n",
    "    ref_chain_id = 'A' if is_ref_alphafold else get_rep_chain_id(uniprot_id, ref_pdb_id, df_rep_chain)\n",
    "    if not ref_chain_id: return [], bw_cache\n",
    "    _, ref_uniprot_to_pdb_map = load_structure_and_map(ref_pdb_id_for_loading, ref_chain_id, uniprot_seq, is_alphafold=is_ref_alphafold)\n",
    "    if not ref_uniprot_to_pdb_map: return [], bw_cache\n",
    "    ref_original_ca_coords = {pos: res['CA'].get_coord() for pos, res in ref_uniprot_to_pdb_map.items()}\n",
    "    \n",
    "    all_pdbs = {pdb: 1 for pdb in gpcr_row['agonist_bound_PDBs']}\n",
    "    all_pdbs.update({pdb: 0 for pdb in gpcr_row['antagonist_bound_PDBs']})\n",
    "    gpcr_features_list = []\n",
    "\n",
    "    for pdb_id, label in all_pdbs.items():\n",
    "        chain_id = get_rep_chain_id(uniprot_id, pdb_id, df_rep_chain)\n",
    "        if not chain_id: continue\n",
    "        target_structure, target_uniprot_to_pdb_map = load_structure_and_map(pdb_id, chain_id, uniprot_seq)\n",
    "        if not target_uniprot_to_pdb_map: continue\n",
    "\n",
    "        common_uniprot_positions = sorted(list(set(ref_original_ca_coords.keys()) & set(target_uniprot_to_pdb_map.keys())))\n",
    "        if len(common_uniprot_positions) < 20: continue\n",
    "        \n",
    "        ref_atoms_for_superimposer = [ref_uniprot_to_pdb_map[pos]['CA'] for pos in common_uniprot_positions]\n",
    "        target_atoms_for_superimposer = [target_uniprot_to_pdb_map[pos]['CA'] for pos in common_uniprot_positions]\n",
    "        \n",
    "        super_imposer = Superimposer()\n",
    "        super_imposer.set_atoms(ref_atoms_for_superimposer, target_atoms_for_superimposer)\n",
    "        super_imposer.apply(target_structure.get_atoms())\n",
    "        \n",
    "        pdb_feature_dict = {'PDB_ID': pdb_id, 'UniProt_ID': uniprot_id, 'Label': label}\n",
    "        target_res_R3_50 = target_uniprot_to_pdb_map.get(uniprot_pos_R3_50)\n",
    "        if not target_res_R3_50: continue\n",
    "\n",
    "        for uni_pos, target_res in target_uniprot_to_pdb_map.items():\n",
    "            bw_num = uniprot_to_bw.get(uni_pos)\n",
    "            if bw_num is None: continue\n",
    "            \n",
    "            dist = np.linalg.norm(target_res['CA'].get_coord() - target_res_R3_50['CA'].get_coord())\n",
    "            pdb_feature_dict[f\"Dist_R3.50_{bw_num}\"] = dist\n",
    "            \n",
    "            original_ref_coord = ref_original_ca_coords.get(uni_pos)\n",
    "            if original_ref_coord is not None:\n",
    "                displacement = np.linalg.norm(target_res['CA'].get_coord() - original_ref_coord)\n",
    "                pdb_feature_dict[f\"Disp_{bw_num}\"] = displacement\n",
    "        gpcr_features_list.append(pdb_feature_dict)\n",
    "    \n",
    "    return gpcr_features_list, bw_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "241cc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK (with final post-processing)\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    logging.info(\"--- ðŸš€ Starting GPCR Decision Tree Feature Matrix Generation (v9) ---\")\n",
    "    \n",
    "    try:\n",
    "        df_class_assigned = pd.read_csv(CLASSIFICATION_FILE)\n",
    "        df_rep_chain = pd.read_csv(REP_CHAIN_FILE)\n",
    "        df_seq_info = pd.read_csv(SEQUENCE_INFO_FILE)\n",
    "        df_class_info = pd.read_csv(CLASS_INFO_FILE)\n",
    "        df_rep_apo = pd.read_csv(REP_APO_FILE)\n",
    "        df_class_assigned['agonist_bound_PDBs'] = df_class_assigned['agonist_bound_PDBs'].apply(eval)\n",
    "        df_class_assigned['antagonist_bound_PDBs'] = df_class_assigned['antagonist_bound_PDBs'].apply(eval)\n",
    "    except FileNotFoundError as e:\n",
    "        logging.critical(f\"FATAL: Input file not found - {e}.\")\n",
    "        return\n",
    "\n",
    "    ref_apo_map = { uniprot: get_representative_apo(uniprot, df_rep_apo) for uniprot in df_class_assigned['GPCR'].unique() }\n",
    "    \n",
    "    df_class_info_subset = df_class_info[['UniProt Accessions', 'Class']]\n",
    "    df_class_assigned = pd.merge(df_class_assigned, df_class_info_subset, left_on='GPCR', right_on='UniProt Accessions', how='left')\n",
    "    df_class_assigned['Class'].fillna('Unknown', inplace=True)\n",
    "    class_dummies = pd.get_dummies(df_class_assigned['Class'], prefix='is_class', dtype=int)\n",
    "    df_class_assigned = pd.concat([df_class_assigned, class_dummies], axis=1).drop(columns=['UniProt Accessions', 'Class'])\n",
    "    logging.info(\"Successfully merged and one-hot encoded GPCR class information.\")\n",
    "    \n",
    "    all_results_list = []\n",
    "    bw_cache = {}\n",
    "    gpcr_rows_list = df_class_assigned.to_dict('records')\n",
    "\n",
    "    for row in tqdm(gpcr_rows_list, desc=\"Processing GPCRs\"):\n",
    "        gpcr_pdb_features, bw_cache = process_gpcr(row, df_rep_chain, df_seq_info, ref_apo_map, bw_cache)\n",
    "        if gpcr_pdb_features:\n",
    "            class_feature_cols = [c for c in row if c.startswith('is_class_')]\n",
    "            for pdb_dict in gpcr_pdb_features:\n",
    "                for col in class_feature_cols: pdb_dict[col] = row[col]\n",
    "            all_results_list.extend(gpcr_pdb_features)\n",
    "\n",
    "    if not all_results_list:\n",
    "        logging.error(\"No features could be extracted. Please check the log file for detailed errors.\")\n",
    "        return\n",
    "        \n",
    "    df_raw_matrix = pd.DataFrame(all_results_list)\n",
    "    logging.info(f\"Successfully generated raw feature matrix. Shape: {df_raw_matrix.shape}\")\n",
    "\n",
    "    logging.info(\"\\n--- Starting Data Cleaning and Finalization ---\")\n",
    "    \n",
    "    id_cols = ['UniProt_ID', 'PDB_ID', 'Label']\n",
    "    feature_cols = [col for col in df_raw_matrix.columns if col not in id_cols]\n",
    "    df_features = df_raw_matrix[feature_cols]\n",
    "\n",
    "    missing_threshold = 0.20\n",
    "    n_samples = len(df_features)\n",
    "    df_filtered = df_features.dropna(axis=1, thresh=int(n_samples * (1 - missing_threshold)))\n",
    "    \n",
    "    dropped_cols = df_features.shape[1] - df_filtered.shape[1]\n",
    "    logging.info(f\"Feature Filtering: Dropped {dropped_cols} columns with > {missing_threshold*100}% missing values.\")\n",
    "    \n",
    "    if df_filtered.isnull().sum().sum() > 0:\n",
    "        logging.info(\"Imputing remaining missing values with column medians...\")\n",
    "        df_imputed = df_filtered.fillna(df_filtered.median())\n",
    "    else:\n",
    "        logging.info(\"No missing values remained after filtering. Skipping imputation.\")\n",
    "        df_imputed = df_filtered\n",
    "\n",
    "    df_final_matrix = pd.concat([df_raw_matrix[id_cols], df_imputed], axis=1)\n",
    "    logging.info(f\"Final clean matrix shape: {df_final_matrix.shape}\")\n",
    "    \n",
    "    df_final_matrix.to_csv(FEATURE_MATRIX_FILE, index=False)\n",
    "    logging.info(f\"âœ… Final clean feature matrix generation complete. Saved to {FEATURE_MATRIX_FILE}\")\n",
    "    \n",
    "    if not df_final_matrix.empty:\n",
    "        logging.info(\"\\n--- Final Clean DataFrame Sample ---\")\n",
    "        id_cols = ['UniProt_ID', 'PDB_ID', 'Label']\n",
    "        class_cols = sorted([c for c in df_final_matrix.columns if c.startswith('is_class_')])\n",
    "        sample_cols = id_cols + class_cols + [c for c in df_final_matrix.columns if '6.30' in c or '7.53' in c][:4]\n",
    "        sample_cols = [c for c in sample_cols if c in df_final_matrix.columns]\n",
    "        logging.info(f\"Final DataFrame Shape: {df_final_matrix.shape}\")\n",
    "        logging.info(df_final_matrix[sample_cols].head().to_string())\n",
    "    else:\n",
    "        logging.warning(\"Final feature matrix is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89812810",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e362b78",
   "metadata": {},
   "source": [
    "Decision Tree modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320672ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Overview ---\n",
      "Total samples for training: 617\n",
      "Number of features: 445\n",
      "-------------------------\n",
      "\n",
      "--- Model Training (on 100% of data) ---\n",
      "Training a full-depth Decision Tree...\n",
      "Training complete.\n",
      "-------------------------\n",
      "\n",
      "--- Full Decision Tree Visualization (using Graphviz) ---\n",
      "Tree visualization saved to '../Output/Decision_Tree/decision_tree_full_visualization_graphviz_v2.png'\n",
      "Please check the new file. It should be rendered correctly.\n",
      "\n",
      "--- Model Performance Evaluation (on Full Training Set) ---\n",
      "Training Accuracy: 1.0000\n",
      "\n",
      "Classification Report (on training data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Antagonist       1.00      1.00      1.00       217\n",
      "   Agagonist       1.00      1.00      1.00       400\n",
      "\n",
      "    accuracy                           1.00       617\n",
      "   macro avg       1.00      1.00      1.00       617\n",
      "weighted avg       1.00      1.00      1.00       617\n",
      "\n",
      "-------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "âœ… The actual full depth of the trained tree is: 17 levels.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import graphviz\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '/home/hyojin0912/Activity/Output/Decision_Tree/GPCR_feature_matrix_for_tree.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "X = df.drop(columns=['UniProt_ID', 'PDB_ID', 'Label'])\n",
    "y = df['Label']\n",
    "\n",
    "print(\"--- Data Overview ---\")\n",
    "print(f\"Total samples for training: {len(df)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- 2. Train the Decision Tree Model on 100% of the Data ---\n",
    "dt_full_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "print(\"\\n--- Model Training (on 100% of data) ---\")\n",
    "print(\"Training a full-depth Decision Tree...\")\n",
    "dt_full_classifier.fit(X, y)\n",
    "print(\"Training complete.\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- 3. Visualize the Tree using Graphviz ---\n",
    "print(\"\\n--- Full Decision Tree Visualization (using Graphviz) ---\")\n",
    "\n",
    "# Export the trained tree to a DOT format string\n",
    "# We still set max_depth=5 to keep the visualization interpretable\n",
    "dot_data = export_graphviz(\n",
    "    dt_full_classifier,\n",
    "    out_file=None, # Output to string instead of file\n",
    "    feature_names=X.columns,\n",
    "    class_names=['Antagonist-bound', 'Agonist-bound'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    "    max_depth=3\n",
    ")\n",
    "\n",
    "# Create a graph from the DOT data\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# Render and save the graph to a file. This will create a high-quality PNG.\n",
    "output_filename = \"../Output/Decision_Tree/decision_tree_full_visualization_graphviz_v2\"\n",
    "graph.render(output_filename, format='png', cleanup=True)\n",
    "print(f\"Tree visualization saved to '{output_filename}.png'\")\n",
    "print(\"Please check the new file. It should be rendered correctly.\")\n",
    "\n",
    "# --- 4. Evaluate Model Performance on the Training Data ---\n",
    "print(\"\\n--- Model Performance Evaluation (on Full Training Set) ---\")\n",
    "y_pred_train = dt_full_classifier.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred_train)\n",
    "class_report = classification_report(y, y_pred_train, target_names=['Antagonist', 'Agagonist'])\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (on training data):\")\n",
    "print(class_report)\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- 5. Check the ACTUAL Full Depth of the Tree ---\n",
    "full_depth = dt_full_classifier.get_depth()\n",
    "print(f\"\\n--------------------------------------------------\")\n",
    "print(f\"âœ… The actual full depth of the trained tree is: {full_depth} levels.\")\n",
    "print(f\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepREAL",
   "language": "python",
   "name": "deepreal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
